{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOmmtoHfhQddAjOwYbg90Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adams-gc/BF-python-class/blob/main/Untitled26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7xC4PteiV19"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "#Models ML\n",
        "from sklearn.svm import SVC\n",
        "#Metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "# from sklearn.metrics import mean_squared_error,r2_score\n",
        "# from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/bankloan.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "L-W5VBJ-mF1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "Nl8mMisbmUuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "MwqcCHjGmV3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for get dummy datata type   (only 2 columna)other wise used ordinal encoding\n",
        "# dependent var not do get dummy (used ordinal encoding ) but independent var get dummy"
      ],
      "metadata": {
        "id": "xAsB-3j6mYyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so preffred ordinal encodeing"
      ],
      "metadata": {
        "id": "S2OXv4PlnLvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "KDm0-8EGo6Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(data[data['Loan_Status'] == 1]))\n",
        "# print(len(data[data['Loan_Status'] == 0]))"
      ],
      "metadata": {
        "id": "xw0OesBjpKpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: do ordinal encoding\n",
        "\n",
        "# from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# # Assuming 'data' DataFrame is already loaded as in your previous code\n",
        "\n",
        "# # Identify columns for ordinal encoding\n",
        "# categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# # Create an OrdinalEncoder object\n",
        "# encoder = OrdinalEncoder()\n",
        "\n",
        "# # Fit and transform the selected columns\n",
        "# data[categorical_cols] = encoder.fit_transform(data[categorical_cols]).astype('bool').astype(int)\n",
        "\n",
        "# # Now 'data' has ordinal encoded categorical columns\n",
        "# print(data.head())"
      ],
      "metadata": {
        "id": "l6_7C_i8n5eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Columns to encode\n",
        "# columns_to_encode = ['Gender', 'Married', 'Dependents', 'Education',\n",
        "#                      'Self_Employed', 'Property_Area', 'Loan_Status']\n",
        "\n",
        "# # Initialize OrdinalEncoder\n",
        "# encoder = OrdinalEncoder()\n",
        "\n",
        "# # Fit and transform the data\n",
        "# data[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\n",
        "\n",
        "# print(data.head())"
      ],
      "metadata": {
        "id": "OgDToDslzphp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adata.describe()"
      ],
      "metadata": {
        "id": "97l8kcRQodVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.isnull().sum()"
      ],
      "metadata": {
        "id": "kF9RO-4ptz1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "cbMGioVNt8Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.isnull().sum()"
      ],
      "metadata": {
        "id": "zxpqrcjtuBIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAKbI3fVuBlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(data[data['Loan_Status'] == 1]))\n",
        "# print(len(data[data['Loan_Status'] == 0]))  #"
      ],
      "metadata": {
        "id": "T3pyJJmGors_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ones = data[data['Loan_Status'] == 1]\n",
        "# zeros = data[data['Loan_Status'] == 0].iloc[0:192]"
      ],
      "metadata": {
        "id": "ry1py04AozIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.concat([ones,zeros]).sample(frac = 1)"
      ],
      "metadata": {
        "id": "WCkrDY7MqCXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.columns"
      ],
      "metadata": {
        "id": "XNHfBTT5qrBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # drop the unwant data from data\n",
        "# data = data.drop(['Loan_ID'], axis = 1)\n",
        "# data"
      ],
      "metadata": {
        "id": "syP12sQJqM96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(10,9))\n",
        "# corr = data.corr()\n",
        "# sns.heatmap(corr, annot=True, cmap='Set2')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "wEr1iLPbq05D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# good correlation :loan amount,applicate income, selfemployeement ,dependes,marital ,gender,loanstatus,"
      ],
      "metadata": {
        "id": "_ixNLbmxq6p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # training and normalization of data\n",
        "# X = data.iloc[:,:-1]\n",
        "# Y = data.iloc[:,-1:]\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state=0)\n",
        "\n",
        "# # SScaler = StandardScaler()\n",
        "# # X_train = SScaler.fit_transform(X_train)\n",
        "# # X_test = SScaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "C21JqZxdtiFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def roc_curve(Y_test, Y_score):\n",
        "#     from sklearn.metrics import roc_curve, auc\n",
        "#     fpr, tpr, thresholds = roc_curve(Y_test, Y_score)\n",
        "#     score = metrics.auc(fpr, tpr)\n",
        "\n",
        "#     fig = px.area(\n",
        "#         #fpr = False Positive Rate; tpr= True Positive Rate\n",
        "#         x=fpr, y=tpr,\n",
        "#         title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
        "#         labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
        "#         width=700, height=500\n",
        "#     )\n",
        "\n",
        "#     fig.add_shape(\n",
        "#         type='line', line=dict(dash='dash'),\n",
        "#         x0=0, x1=1, y0=0, y1=1\n",
        "#     )\n",
        "\n",
        "#     fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "#     fig.update_xaxes(constrain='domain')\n",
        "#     fig.show()"
      ],
      "metadata": {
        "id": "JPN_huWY0mwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVC\n",
        "# svc= SVC()\n",
        "# svc.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "mf2h7oD-tjN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y_pred = svc.predict(X_test)\n",
        "\n",
        "# svc_accuracy= round(accuracy_score(Y_test,Y_pred), 5)*100 # Accuracy\n",
        "# svc_accuracy\n"
      ],
      "metadata": {
        "id": "IwJZrZGZtq-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d = X_test.iloc[0:1]\n",
        "# d"
      ],
      "metadata": {
        "id": "uEot5yb4xd-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y_test.iloc[0:1]"
      ],
      "metadata": {
        "id": "esgSQWzoxigt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y_score = svc.predict(d)\n",
        "# print(Y_score)"
      ],
      "metadata": {
        "id": "nIvMV2Pv0JW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: do confusion matric\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# # Assuming Y_test and Y_pred are already defined from your previous code\n",
        "\n",
        "# cm = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#             xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "#             yticklabels=['Actual 0', 'Actual 1'])\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.xlabel('Predicted Label')\n",
        "# plt.ylabel('True Label')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "DubfvIVu2i_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "in1EvEj32Y09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # def impressions(model,accuracy):\n",
        "# #     print('Accuracy: {} %'.format(accuracy))\n",
        "#     # print('Mean squared error: ', round(mean_squared_error(Y_test,Y_pred),3))\n",
        "# print(\"All test dataset rows = \",len(X_test))\n",
        "# cm=confusion_matrix(Y_test,Y_pred)\n",
        "# class_label = [0, 1]\n",
        "# df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n",
        "# sns.heatmap(df_cm,annot=True,cmap='Set2',linewidths=2,fmt='d')\n",
        "# plt.title(\"Confusion Matrix\",fontsize=15)\n",
        "# plt.xlabel(\"Predicted\")\n",
        "# plt.ylabel(\"True\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "s8dvmR6D1Gh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# ytest = np.array(Y_test)\n",
        "# print(classification_report(ytest,svc.predict(X_test)))"
      ],
      "metadata": {
        "id": "i5hztfYX6Heb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm=confusion_matrix(Y_test,Y_pred)\n",
        "# cm"
      ],
      "metadata": {
        "id": "eAHRvmx169DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(len(Y_test[Y_test['Loan_Status'] == True]))\n",
        "# print(len(Y_test[Y_test['Loan_Status'] == False]))"
      ],
      "metadata": {
        "id": "YGJlah1W5HF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.drop"
      ],
      "metadata": {
        "id": "q6qDjG-7WQu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1: Data Cleaning and Preparation\n",
        "\n",
        "# # Copy the dataset to avoid modifying the original\n",
        "# df = data.copy()\n",
        "\n",
        "# # Drop the Loan_ID column as it is not useful for modeling\n",
        "# df.drop(columns=['Loan_ID'], inplace=True)\n",
        "\n",
        "# # Fill missing values for categorical columns with the mode (most frequent value)\n",
        "# categorical_columns = ['Gender', 'Married', 'Self_Employed', 'Dependents', 'Credit_History', 'Loan_Amount_Term']\n",
        "# for column in categorical_columns:\n",
        "#     df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "\n",
        "# # Fill missing values for numerical columns with the median (to reduce the effect of outliers)\n",
        "# numerical_columns = ['LoanAmount']\n",
        "# for column in numerical_columns:\n",
        "#     df[column].fillna(df[column].median(), inplace=True)\n",
        "\n",
        "# # Convert the target variable Loan_Status to binary (1 for \"Y\" and 0 for \"N\")\n",
        "# df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
        "\n",
        "# # Convert Dependents column to a string type for encoding purposes\n",
        "# df['Dependents'] = df['Dependents'].astype(str)\n",
        "\n",
        "# # Step 2: Encoding Categorical Variables\n",
        "# # Use One-Hot Encoding for categorical variables\n",
        "# df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# # Step 3: Splitting the Data into Training and Testing Sets\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Define features (X) and target (y)\n",
        "# X = df.drop(columns=['Loan_Status'])\n",
        "# y = df['Loan_Status']\n",
        "\n",
        "# # Split the dataset into 80% training and 20% testing data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# # Step 4: Standardizing the Features\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Standardize numerical columns for better performance of SVM\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Step 5: Train an SVM Model\n",
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Instantiate an SVM model with default settings\n",
        "# svm_model = SVC(kernel='linear', random_state=42)\n",
        "# svm_model.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# # Step 6: Evaluate the Model\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# # Generate a confusion matrix\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # Generate a classification report\n",
        "# class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# # Display the confusion matrix and classification report\n",
        "# conf_matrix, class_report\n"
      ],
      "metadata": {
        "id": "TafbraYY5Xp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll perform a complete data science workflow on the dataset . Here's the typical process I'll follow:\n",
        "\n",
        "### Data Science Workflow:\n",
        "1. **Problem Definition**: Understand the context and goals.\n",
        "2. **Data Inspection**: Load and examine the dataset.\n",
        "3. **Data Preprocessing**: Clean, transform, and prepare the data.\n",
        "4. **Exploratory Data Analysis (EDA)**: Discover patterns and insights.\n",
        "5. **Feature Engineering**: Select or create features for modeling.\n",
        "6. **Model Building**: Train and evaluate machine learning models.\n",
        "7. **Model Evaluation**: Compare models and select the best one.\n",
        "8. **Interpretation**: Interpret results and draw conclusions.\n",
        "9. **Documentation**: Provide insights and recommendations."
      ],
      "metadata": {
        "id": "4-J4S7gzaZFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Problem Definition\n",
        "\n",
        "The dataset  `bankloan.csv`. Based on the name, it is bank loan data, which could involve predicting loan approvals, analyzing risk factors, or customer profiling. Let's define a hypothetical objective:\n",
        "\n",
        "#### Hypothetical Objective:\n",
        "To build a predictive model that determines whether a loan will be approved based on customer and loan attributes. This could help banks make data-driven decisions, reduce risk, and enhance customer service.\n",
        "\n",
        "#### Potential Target Variable:\n",
        "From similar datasets, the target variable could be:\n",
        "- **Loan Status**: Approved/Not Approved.\n",
        "\n",
        "#### Potential Predictors:\n",
        "- Customer demographics (age, income, gender, etc.).\n",
        "- Loan details (amount, tenure, interest rate, etc.).\n",
        "- Credit history or score.\n",
        "\n",
        "\n",
        "The dataset contains 614 entries and 13 columns, with a mix of categorical and numerical data. Here's a summary of its structure:\n",
        "\n",
        "### Key Observations:\n",
        "1. **Columns**:\n",
        "   - **Categorical**: `Gender`, `Married`, `Education`, `Self_Employed`, `Property_Area`, `Loan_Status`.\n",
        "   - **Numerical**: `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, `Loan_Amount_Term`, `Credit_History`, `Dependents`.\n",
        "\n",
        "2. **Target Variable**:\n",
        "   - `Loan_Status`: Indicates whether the loan was approved (`Y`/`N`).\n",
        "\n",
        "3. **Missing Values**:\n",
        "   - Several columns have missing values, such as `LoanAmount`, `Credit_History`, and `Gender`.\n",
        "\n",
        "4. **Potential Features**:\n",
        "   - `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, and other attributes seem relevant for predicting loan approval.\n"
      ],
      "metadata": {
        "id": "6ekcNwzua3cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information and the first few rows\n",
        "data_info = data.info()\n",
        "data_head = data.head()\n",
        "\n",
        "data_info, data_head"
      ],
      "metadata": {
        "id": "Yy_TkIPq9FtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Data Inspection\n",
        "\n",
        "#### Objectives:\n",
        "1. Summarize the dataset for missing values, unique values, and data distributions.\n",
        "2. Understand the relationships between variables.\n",
        "\n",
        "#### Actions:\n",
        "1. **Check for missing values**: Identify and quantify missing data in each column.\n",
        "2. **Explore data distributions**: Summarize numerical and categorical data for patterns and anomalies.\n",
        "3. **Identify relationships**: Spot correlations and dependencies between variables.\n",
        "\n",
        "\n",
        "\n",
        "### Data Inspection Summary:\n",
        "\n",
        "#### 1. **Missing Values**:\n",
        "- Significant missing values in:\n",
        "  - `Gender`: 13 missing values.\n",
        "  - `Married`: 3 missing values.\n",
        "  - `Dependents`: 15 missing values.\n",
        "  - `Self_Employed`: 32 missing values.\n",
        "  - `LoanAmount`: 22 missing values.\n",
        "  - `Loan_Amount_Term`: 14 missing values.\n",
        "  - `Credit_History`: 50 missing values.\n",
        "\n",
        "#### 2. **Numerical Data**:\n",
        "- `ApplicantIncome`: Wide range (min: 150, max: 81,000) with a high standard deviation, indicating potential outliers.\n",
        "- `CoapplicantIncome`: Many entries are 0, likely representing no co-applicant.\n",
        "- `LoanAmount`: Average is 146, with a range from 9 to 700.\n",
        "- `Loan_Amount_Term`: Predominantly 360 months, indicating long-term loans.\n",
        "- `Credit_History`: Binary values (0 or 1), but 50 missing entries.\n",
        "\n",
        "#### 3. **Categorical Data**:\n",
        "- `Loan_ID`: Unique identifier (not useful for modeling).\n",
        "- `Gender`, `Married`, `Education`, `Self_Employed`, and `Property_Area`: Limited categories.\n",
        "  - Most frequent values:\n",
        "    - `Gender`: Male (489 occurrences).\n",
        "    - `Married`: Yes (398 occurrences).\n",
        "    - `Education`: Graduate (480 occurrences).\n",
        "    - `Property_Area`: Semiurban (233 occurrences).\n",
        "- `Loan_Status`: Target variable; Yes (422 occurrences) and No (192 occurrences).\n",
        "\n",
        "# 4. Unique Values:\n",
        "- `Loan_ID`: 614 unique entries (used for tracking, not modeling).\n",
        "- Categorical predictors generally have 2â€“4 unique values.\n"
      ],
      "metadata": {
        "id": "VhBltwiacNEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Summarize numerical columns\n",
        "numerical_summary = data.describe()\n",
        "\n",
        "# Summarize categorical columns\n",
        "categorical_summary = data.select_dtypes(include='object').describe()\n",
        "\n",
        "# Unique values per column\n",
        "unique_values = data.nunique()\n",
        "\n",
        "missing_values, numerical_summary, categorical_summary, unique_values\n"
      ],
      "metadata": {
        "id": "-APYurI-a2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Preprocessing\n",
        "\n",
        "#### Objectives:\n",
        "1. **Handle Missing Values**: Fill or remove missing values based on the data type and context.\n",
        "2. **Encode Categorical Variables**: Transform categorical data into a numerical format suitable for machine learning models.\n",
        "3. **Outlier Handling**: Address extreme values that could distort the analysis.\n",
        "4. **Feature Scaling**: Standardize numerical data if required for modeling.\n",
        "\n",
        "#### Actions:\n",
        "1. **Handling Missing Values**:\n",
        "   - Impute categorical variables with the mode.\n",
        "   - Impute numerical variables with the mean or median based on distribution.\n",
        "\n",
        "2. **Encode Categorical Variables**:\n",
        "   - Use one-hot encoding for categorical columns like `Property_Area`.\n",
        "   - Use label encoding for binary columns like `Loan_Status`.\n",
        "\n",
        "3. **Address Outliers**:\n",
        "   - Inspect `ApplicantIncome` and `LoanAmount` for extreme values.\n",
        "\n",
        "4. **Feature Scaling**:\n",
        "   - Standardize numerical features if required for machine learning models.\n"
      ],
      "metadata": {
        "id": "LW6LUy2dcqsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Handle missing values\n",
        "# Categorical variables: Fill with mode\n",
        "categorical_columns = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History', 'Loan_Amount_Term']\n",
        "for col in categorical_columns:\n",
        "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "\n",
        "# Numerical variables: Fill with mean\n",
        "numerical_columns = ['LoanAmount']\n",
        "for col in numerical_columns:\n",
        "    data[col].fillna(data[col].mean(), inplace=True)\n",
        "\n",
        "# 2. Encode categorical variables\n",
        "# Label encoding for binary variables\n",
        "label_encoder = LabelEncoder()\n",
        "data['Loan_Status'] = label_encoder.fit_transform(data['Loan_Status'])\n",
        "\n",
        "# One-hot encoding for multi-category columns\n",
        "data = pd.get_dummies(data, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents'])\n",
        "\n",
        "# 3. Handle outliers\n",
        "# Cap extreme values in 'ApplicantIncome' and 'LoanAmount'\n",
        "income_cap = data['ApplicantIncome'].quantile(0.99)\n",
        "loan_amount_cap = data['LoanAmount'].quantile(0.99)\n",
        "data['ApplicantIncome'] = data['ApplicantIncome'].clip(upper=income_cap)\n",
        "data['LoanAmount'] = data['LoanAmount'].clip(upper=loan_amount_cap)\n",
        "\n",
        "# 4. Feature scaling (if needed for ML models, e.g., SVM)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
        "data[scaled_columns] = scaler.fit_transform(data[scaled_columns])\n",
        "\n",
        "# Display the cleaned dataset\n",
        "data.info(), data.head()\n"
      ],
      "metadata": {
        "id": "9nVeO5ZtaQkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Exploratory Data Analysis (EDA)\n",
        "\n",
        "#### Objectives:\n",
        "1. **Understand Target Distribution**: Analyze the balance of `Loan_Status`.\n",
        "2. **Visualize Relationships**: Examine relationships between features and the target variable.\n",
        "3. **Correlation Analysis**: Identify features strongly correlated with `Loan_Status`.\n",
        "4. **Feature Importance**: Highlight features that could significantly influence predictions.\n",
        "### Key Insights:\n",
        "1. **Target Distribution**:\n",
        "   - Visualize the balance between `Loan_Status = 0` (Not Approved) and `Loan_Status = 1` (Approved).\n",
        "\n",
        "2. **Numerical Features**:\n",
        "   - Check how income and loan amount vary with loan approval.\n",
        "\n",
        "3. **Categorical Features**:\n",
        "   - Explore patterns in `Gender`, `Married`, `Education`, etc., against loan approval.\n",
        "\n",
        "4. **Feature Correlations**:\n",
        "   - Highlight numerical features most correlated with `Loan_Status`.\n"
      ],
      "metadata": {
        "id": "REtT5iZ9dIko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# 1. Target Distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Loan_Status', data=data)\n",
        "plt.title(\"Distribution of Loan_Status\")\n",
        "plt.xlabel(\"Loan Status (0 = Not Approved, 1 = Approved)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Distribution of Numerical Features\n",
        "numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
        "data[numerical_columns].hist(bins=20, figsize=(12, 6))\n",
        "plt.suptitle(\"Distributions of Numerical Features\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Relationship Between Numerical Features and Target\n",
        "for col in numerical_columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x='Loan_Status', y=col, data=data)\n",
        "    plt.title(f\"Relationship between {col} and Loan_Status\")\n",
        "    plt.xlabel(\"Loan Status (0 = Not Approved, 1 = Approved)\")\n",
        "    plt.ylabel(col)\n",
        "    plt.show()\n",
        "\n",
        "# 4. Categorical Feature Analysis\n",
        "categorical_columns = [col for col in data.columns if data[col].dtype == 'object' and col != 'Loan_ID']\n",
        "for col in categorical_columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x=col, hue='Loan_Status', data=data)\n",
        "    plt.title(f\"{col} vs Loan_Status\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend(title=\"Loan Status\", labels=[\"Not Approved\", \"Approved\"])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 5. Correlation Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Exclude non-numeric columns before calculating correlation\n",
        "numerical_data = data.select_dtypes(include=['number'])  # Select only numerical columns\n",
        "correlation = numerical_data.corr()\n",
        "sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Feature Importance using Correlation with Loan_Status\n",
        "correlation_target = correlation['Loan_Status'].sort_values(ascending=False)\n",
        "print(\"Correlation with Loan_Status:\")\n",
        "print(correlation_target)\n"
      ],
      "metadata": {
        "id": "hIvmj01uc7En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Feature Engineering\n",
        "\n",
        "#### Objectives:\n",
        "1. **Select Relevant Features**: Identify features critical to predicting the target.\n",
        "2. **Create New Features**: Derive new meaningful features to enhance predictive power.\n",
        "3. **Transform Features**: Ensure all features are suitable for machine learning models.\n",
        "\n",
        "\n",
        "### Key Features Added:\n",
        "1. **Income-to-Loan Ratio**:\n",
        "   - Helps assess whether the loan requested is proportionate to the applicant's income.\n",
        "\n",
        "2. **Total Income**:\n",
        "   - Combines `ApplicantIncome` and `CoapplicantIncome` for a holistic view of income.\n",
        "\n",
        "3. **Income Category**:\n",
        "   - Categorizes income into bins (Low, Medium, High, Very High).\n",
        "\n",
        "4. **Transformed Features**:\n",
        "   - Scales skewed numerical variables using a power transformation.\n"
      ],
      "metadata": {
        "id": "I16P1gYufgjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Feature Selection\n",
        "# Drop irrelevant features (e.g., unique identifiers)\n",
        "data = data.drop(columns=['Loan_ID'], axis=1)\n",
        "\n",
        "# 2. Create New Features\n",
        "# Add a ratio of ApplicantIncome to LoanAmount\n",
        "data['Income_to_Loan_Ratio'] = data['ApplicantIncome'] / (data['LoanAmount'] + 1)\n",
        "\n",
        "# Add a TotalIncome feature combining Applicant and Coapplicant Income\n",
        "data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n",
        "\n",
        "# Categorize Total Income into bins (Low, Medium, High)\n",
        "data['Income_Category'] = pd.cut(\n",
        "    data['TotalIncome'],\n",
        "    bins=[0, 2500, 4000, 8000, float('inf')],\n",
        "    labels=['Low', 'Medium', 'High', 'Very High']\n",
        ")\n",
        "\n",
        "# Encode new categorical features if required\n",
        "data = pd.get_dummies(data, columns=['Income_Category'], drop_first=True)\n",
        "\n",
        "# 3. Transform Features\n",
        "# Normalize skewed features (if needed, especially for tree-based models)\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "pt = PowerTransformer()\n",
        "data[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']] = pt.fit_transform(\n",
        "    data[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\n",
        ")\n",
        "\n",
        "# Feature Summary\n",
        "print(\"Feature Engineering Complete!\")\n",
        "print(f\"New Features Added: Income_to_Loan_Ratio, TotalIncome, Income_Category\")\n",
        "print(f\"Final Shape of Dataset: {data.shape}\")\n",
        "\n",
        "# Save the updated dataset for modeling\n",
        "data.to_csv(\"bankloan_features.csv\", index=False)\n",
        "print(\"Feature-engineered dataset saved as 'bankloan_features.csv'\")\n"
      ],
      "metadata": {
        "id": "OKaj9DTIeNcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Model Building\n",
        "\n",
        "#### Objectives:\n",
        "1. **Split the Data**: Divide the dataset into training and testing sets.\n",
        "2. **Train Models**: Use common classification models to predict `Loan_Status`.\n",
        "3. **Evaluate Models**: Measure performance using metrics like accuracy, precision, recall, and F1-score.\n",
        "4. **Optimize the Model**: Apply hyperparameter tuning to improve performance.\n",
        "\n",
        "\n",
        "\n",
        "### Code Walkthrough:\n",
        "1. **Data Splitting**:\n",
        "   - The dataset is split into features (`X`) and the target (`y`).\n",
        "   - An 80-20 split is used for training and testing.\n",
        "\n",
        "2. **Model Training**:\n",
        "   - Three models are trained: Logistic Regression, Random Forest, and Support Vector Machine (SVM).\n",
        "\n",
        "3. **Model Evaluation**:\n",
        "   - Each model is evaluated using accuracy, a classification report, and a confusion matrix.\n",
        "\n",
        "4. **Model Saving**:\n",
        "   - The best-performing model (e.g., Random Forest) is saved as `best_model.pkl`.\n"
      ],
      "metadata": {
        "id": "wpAuu53pgBm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the feature-engineered dataset\n",
        "data = pd.read_csv(\"bankloan_features.csv\")\n",
        "\n",
        "# 1. Split the data into features and target\n",
        "X = data.drop(columns=['Loan_Status'], axis=1)\n",
        "y = data['Loan_Status']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train multiple models\n",
        "# Logistic Regression\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "\n",
        "# 3. Evaluate Models\n",
        "def evaluate_model(name, y_test, y_pred):\n",
        "    print(f\"--- {name} Performance ---\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluate each model\n",
        "evaluate_model(\"Logistic Regression\", y_test, lr_preds)\n",
        "evaluate_model(\"Random Forest\", y_test, rf_preds)\n",
        "evaluate_model(\"SVM\", y_test, svm_preds)\n",
        "\n",
        "# 4. Save the best model (example: Random Forest)\n",
        "import joblib\n",
        "joblib.dump(rf_model, \"best_model.pkl\")\n",
        "print(\"Best model saved as 'best_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "SePngdRlfwIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdGhozz9gktk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}